{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368e0af-cb73-42ec-bfd6-7ccdfe301cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities for both OpenAI API and local AITA modules. These include 3 functions and two sub-functions:\n",
    "#1. generate_prompt: creating the prompt\n",
    "#2. extract_response: extracting the JSON and summary blocks,\n",
    "#3. output_aita: inserting the JSON from AITA to the CSV, if writing report selected printing the summary to a file.\n",
    "#4. subfunctoins is_valid_json and fix_json - two sub-functoins of output aita, for the purpose of fixing responses with invalid json strings.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "try:\n",
    "    base_path = os.path.dirname(__file__)              # getting the file paths when running in CLI \n",
    "except NameError:\n",
    "    base_path = os.getcwd()                            # getting the file paths when running in Jupyter Notebooks\n",
    "sys.path.append(os.path.join(base_path, 'parser'))\n",
    "sys.path.append(os.path.join(base_path, 'AITA'))\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(base_path, \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Creating the prompt for the LLM. Takes a base prompt from text file and appends the parsed log.\n",
    "\n",
    "def generate_prompt(parsed_log_path, write_report=False):                  \n",
    "\n",
    "    with open(parsed_log_path) as parsed_file:\n",
    "        log_content = parsed_file.read()\n",
    "\n",
    "    prompt_path = os.path.abspath(os.path.join(base_path, \"..\", \"AITA\", \"baseprompt.txt\"))\n",
    "\n",
    "    with open(parsed_log_path) as parsed_file:\n",
    "        log_content = parsed_file.read()\n",
    "\n",
    "    with open(prompt_path, \"r\") as promptfile:\n",
    "        base_prompt = promptfile.read()\n",
    "\n",
    "    prompt = base_prompt+log_content\n",
    "      \n",
    "    if write_report:\n",
    "        prompt += '''\n",
    "        After the JSON block, write the line ---COMMENCE-REPORT---, and then write a brief report (maximum 50 words) summarizing the findings in natural language.\n",
    "        This report must appear on a new line after the JSON and must not be inside the JSON itself.\n",
    "        '''\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Extracting the log lines and the AITA report if report is selected. \n",
    "def extract_response(LLM_output):                              \n",
    "    partitioned = LLM_output.partition(\"---COMMENCE-REPORT---\")\n",
    "    row_list = partitioned[0]\n",
    "    if \"json\" in row_list:\n",
    "        row_list = row_list.replace(\"```json\", \"\")\n",
    "        row_list = row_list.replace(\"```\", \"\")\n",
    "    return row_list.strip(), partitioned[2].strip()\n",
    "\n",
    "# Subfunction checking if string is valid json.\n",
    "def is_valid_json(tested_string):\n",
    "    try:\n",
    "        data = json.loads(tested_string)\n",
    "        valid = True\n",
    "    except json.JSONDecodeError:\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "# Subfunction to fix the string if it is not valid json.\n",
    "def fix_json(string_to_fix):\n",
    "    string_to_fix = string_to_fix.partition(\"---COMMENCE-REPORT---\")[0]\n",
    "    start = string_to_fix.find('[')\n",
    "    end = string_to_fix.rfind(']')\n",
    "    if start != -1 and end != -1 and end > start: string_to_fix=string_to_fix[start:end+1]\n",
    "    string_to_fix.split(']', 1)[0] + ']'\n",
    "    string_to_fix = string_to_fix.strip()\n",
    "    string_to_fix = re.split(r\"-{2,}\\s*COMMENCE-REPORT\\s*-{2,}\", string_to_fix, flags=re.IGNORECASE)[0]\n",
    "\n",
    "    if not string_to_fix.startswith(\"[\"): string_to_fix = \"[\"+string_to_fix\n",
    "    if not string_to_fix.endswith(\"]\"): string_to_fix = string_to_fix+\"]\"\n",
    "    \n",
    "    string_to_fix = string_to_fix.replace(\"{{\", \"{\").replace(\"}}\", \"}\")        \n",
    "    string_to_fix = string_to_fix.replace(\"[[\", \"[\").replace(\"]]\", \"]\")\n",
    "    string_to_fix = re.sub(r\",\\s*]\", \"]\", string_to_fix)\n",
    "    string_to_fix = re.sub(r\",\\s*}\", \"}\", string_to_fix)\n",
    "    string_to_fix = re.sub(r'(?<![\"\\'])\\b(\\w+)\\b(?=\\s*:)', r'\"\\1\"', string_to_fix)\n",
    "    string_to_fix = re.sub(r'^\\[\\s*\"row\":', r'[{\"row\":', string_to_fix)\n",
    "    string_to_fix = re.sub(r'\\}\\s*\\]$', r'}]', string_to_fix)  # just in case it ends weirdly\n",
    "        \n",
    "    string_to_fix = string_to_fix.replace(\"```json\", \"\")\n",
    "    string_to_fix = string_to_fix.replace(\"```\", \"\")\n",
    "    string_to_fix = string_to_fix.replace(\"''\", \"'\")\n",
    "    string_to_fix = string_to_fix.replace('\"\"', '\"')\n",
    "    \n",
    "    return string_to_fix\n",
    "\n",
    "# Outputing the AITA response to CSV and to a report text file if report is selected.\n",
    "def output_AITA(filepath, extracted_response, model=\"None\", time=0, print_mode=False):   \n",
    "    aita_list = extracted_response[0]\n",
    "    if not is_valid_json(aita_list):\n",
    "        aita_list = fix_json(aita_list)\n",
    "    aita_list = json.loads(aita_list)\n",
    "    \n",
    "    for item in aita_list:\n",
    "        item[\"risk_level\"] = int(item[\"risk_level\"])\n",
    "    aita_combined = pd.read_csv(filepath)              # creating a dataframe.\n",
    "    aita_combined[\"aita_risk_level\"] = 1               # dataframe filled with default 1 risk level, no comment.\n",
    "    aita_combined[\"aita_comment\"] = \"\"\n",
    "\n",
    "    for line in aita_list:\n",
    "        row_number = line[\"row\"]\n",
    "        aita_combined.loc[row_number, \"aita_risk_level\"] = line[\"risk_level\"]\n",
    "        aita_combined.loc[row_number, \"aita_comment\"] = line[\"comment\"]\n",
    "    \n",
    "    aita_combined.to_csv(filepath, index=False)        # writing the combined dataframe to csv file.\n",
    "\n",
    "    ### The following segment is in case report mode is selected.\n",
    "    \n",
    "    if extracted_response[1] != \"\":                    # preparing the report text.\n",
    "        report_datetime = str(datetime.now())\n",
    "        report_date, milisecs = report_datetime.rsplit(\".\", 1)\n",
    "        lines_list = \", \".join(str(line[\"row\"]) for line in aita_list)\n",
    "        report = f\"\"\"AI TRIAGE ASSISTANT SUMMARY REPORT:\n",
    "-----------------------------------\n",
    "Report date: {report_date}\n",
    "Analyzed log file: {filepath} | Model: {model} | Time to process: {time:.4f} seconds\n",
    "Number of suspected lines (risk level above 1): {len(aita_list)}\n",
    "Suspected lines: {lines_list}\n",
    "\n",
    "AITA SUMMARY:\n",
    "-------------\n",
    "{extracted_response[1]}\n",
    "\"\"\"\n",
    "        if print_mode: print(report)\n",
    "        \n",
    "        filename_base, ext = filepath.rsplit(\".\", 1)               # Set filename as parsed_log_path-AITA_report.txt, and write report to it.\n",
    "        filename = filename_base + \"-AITA_report.txt\"\n",
    "        with open(filename, \"w\") as reportfile:        \n",
    "            reportfile.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
